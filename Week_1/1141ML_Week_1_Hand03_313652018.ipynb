{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a29d15",
   "metadata": {},
   "source": [
    "# Think\n",
    "There are unanswered questions during the lecture, and there are likely more questions we haven't covered. Take a moment to think about them and write them down here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f3803f",
   "metadata": {},
   "source": [
    "## Unanswered question\n",
    "During the lecture, I noticed that the update formula for gradient descent sometimes includes different coefficients, such as:\n",
    "\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n - \\alpha \\nabla L(\\theta)\n",
    "$$\n",
    "vs.\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n + \\frac{2\\alpha}{m} \\sum (y^{(i)} - h(x^{(i)})) \\nabla_\\theta h\n",
    "$$\n",
    "\n",
    "My question is:  \n",
    "- Why do these formulas look slightly different?  \n",
    "- Is the difference only due to whether the MSE loss is defined with a factor of \\( \\tfrac{1}{2} \\), or whether the gradient is averaged over \\(m\\)?  \n",
    "- If the constants can be absorbed into the learning rate \\(\\alpha\\), does it mean the actual choice of coefficient is not important, as long as \\(\\alpha\\) is tuned properly?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2037a",
   "metadata": {},
   "source": [
    "## Thoughts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ed10d",
   "metadata": {},
   "source": [
    "One open question I have is about the different coefficients in the gradient descent update rule.  \n",
    "Sometimes the update is written as:\n",
    "\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n - \\alpha \\nabla L(\\theta),\n",
    "$$\n",
    "\n",
    "and other times as:\n",
    "\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n + \\frac{2\\alpha}{m} \\sum (y^{(i)} - h(x^{(i)})) \\nabla_\\theta h.\n",
    "$$\n",
    "\n",
    "I wonder:\n",
    "\n",
    "- Is the difference mainly due to how the MSE loss is defined with or without the $\\frac{1}{2}$?  \n",
    "- Does averaging over $m$ samples vs. not averaging only change the scale, which can be absorbed into the learning rate $\\alpha$?  \n",
    "- In practice, does this mean the coefficient difference is mathematically unimportant, but practically it affects how we choose $\\alpha$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c552a16b",
   "metadata": {},
   "source": [
    "### Acknoledgement\n",
    "This question has been put on the ChatGPT. At first, I and my friend thought this difference is just depends on the user's habits; however, we figure out that the choice is probably related to $\\alpha$. Then we asked chatgpt and it helps us to sort up the thoughts."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
