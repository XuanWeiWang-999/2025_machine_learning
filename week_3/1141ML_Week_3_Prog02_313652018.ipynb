{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2be9530c",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "In this file, we aim to make $$\\hat{f(x)}\\thickapprox f(x), \\frac{d\\hat{f(x)}}{dx}\\thickapprox f'(x)$$\n",
    "at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3af88a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bd04c4",
   "metadata": {},
   "source": [
    "# Setting\n",
    "- dataset: sampling method (uniform random over [-1, 1])\n",
    "- dataset size:\n",
    "    - training set: 256\n",
    "    - validation set: 256\n",
    "    - test set: 1000\n",
    "- Neurak network architecture:\n",
    "    - hidden layers: 2\n",
    "    - nonlinear function: tanh\n",
    "    - tanh units: 64\n",
    "- optimizer: Adam\n",
    "- learning rate: $2e-3$\n",
    "- weight decay: $1e-5$\n",
    "- epoches: 800\n",
    "- Loss metric: MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1024836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- config ----------\n",
    "seed = 42\n",
    "n_train, n_val, n_test = 256, 256, 1000\n",
    "hidden = 64\n",
    "lr = 2e-3\n",
    "weight_decay = 1e-5\n",
    "epochs = 800\n",
    "patience = 80\n",
    "device = \"cpu\"\n",
    "\n",
    "# ---------- utils ----------\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "def f(x):\n",
    "    return 1.0 / (1.0 + 25.0 * x**2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5faded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset \n",
    "def chebyshev_nodes(n):\n",
    "    k = np.arange(1, n+1)\n",
    "    return np.cos((2*k - 1) / (2*n) * np.pi).reshape(-1,1)\n",
    "x_train=chebyshev_nodes(n_train)\n",
    "y_train=f(x_train)\n",
    "x_val=chebyshev_nodes(n_val)\n",
    "y_val=f(x_val)\n",
    "x_test=np.linspace(-1.0, 1.0, n_test).reshape(-1,1)\n",
    "y_test=f(x_test)\n",
    "\n",
    "toT = lambda a: torch.from_numpy(a).float().to(device)\n",
    "Xtr, Ytr = toT(x_train), toT(y_train)\n",
    "Xva, Yva = toT(x_val),   toT(y_val)\n",
    "Xte, Yte = toT(x_test),  toT(y_test)\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(Xtr, Ytr), batch_size=64, shuffle=True)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, h=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1,h), nn.Tanh(),\n",
    "            nn.Linear(h,h), nn.Tanh(),\n",
    "            nn.Linear(h,1)\n",
    "        )\n",
    "    def forward(self, x): \n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP(hidden).to(device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "338933a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE      : 1.197438e-07\n",
      "Test Max |err|: 6.388045e-04\n",
      "Test Derivative MSE      : 8.212621e-06\n",
      "Test Derivative Max |err|: 8.734465e-03\n"
     ]
    }
   ],
   "source": [
    "def fprime(x: torch.Tensor):\n",
    "    return (-50.0 * x / (1.0 + 25.0 * x**2)**2).detach()\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.MSELoss()\n",
    "lambda_d = 1.0  # 可調 0.1~10\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "best_val = float(\"inf\")\n",
    "best_state = None\n",
    "since_best = 0\n",
    "patience = 80  # 你原本有設定就沿用\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    # ===== Train =====\n",
    "    model.train()\n",
    "    epoch_train_f = 0.0\n",
    "    epoch_train_d = 0.0\n",
    "    nbatch = 0\n",
    "\n",
    "    for xb, yb in train_dl:\n",
    "        # 讓 x 成為 leaf 可微分\n",
    "        xb = xb.detach().clone().requires_grad_(True)\n",
    "        y_true = yb\n",
    "\n",
    "        # forward\n",
    "        y_pred = model(xb)\n",
    "\n",
    "        # d\\hat f/dx  (建立計算圖，讓 derivative loss 能反傳到參數)\n",
    "        dy_dx = torch.autograd.grad(\n",
    "            outputs=y_pred,\n",
    "            inputs=xb,\n",
    "            grad_outputs=torch.ones_like(y_pred),\n",
    "            create_graph=True,     # << 必要\n",
    "            #retain_graph=False,\n",
    "            #only_inputs=True\n",
    "        )[0]\n",
    "\n",
    "        # losses\n",
    "        loss_f = loss_fn(y_pred, y_true)\n",
    "        loss_d = loss_fn(dy_dx, fprime(xb))\n",
    "        loss = loss_f + lambda_d * loss_d\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()           # 只 backward 一次\n",
    "        opt.step()\n",
    "\n",
    "        epoch_train_f += loss_f.item()\n",
    "        epoch_train_d += loss_d.item()\n",
    "        nbatch += 1\n",
    "\n",
    "    # 平均一下\n",
    "    train_f = epoch_train_f / nbatch\n",
    "    train_d = epoch_train_d / nbatch\n",
    "    train_total = train_f + lambda_d * train_d\n",
    "\n",
    "    # ===== Validate =====\n",
    "    model.eval()\n",
    "\n",
    "    # 我們要對 xv 求導，但不需要對參數求梯度 → 暫時關閉參數梯度，省顯存\n",
    "    toggled = []\n",
    "    for p in model.parameters():\n",
    "        if p.requires_grad:\n",
    "            p.requires_grad_(False)\n",
    "            toggled.append(p)\n",
    "\n",
    "    xv = Xva.detach().clone().requires_grad_(True)\n",
    "    y_pred_v = model(xv)\n",
    "\n",
    "    # 對 xv 取導，create_graph=False（驗證不反傳）\n",
    "    dv = torch.autograd.grad(\n",
    "        outputs=y_pred_v,\n",
    "        inputs=xv,\n",
    "        grad_outputs=torch.ones_like(y_pred_v),\n",
    "        create_graph=False,\n",
    "        retain_graph=False,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    val_loss_f = loss_fn(y_pred_v, Yva).item()\n",
    "    val_loss_d = loss_fn(dv, fprime(xv)).item()\n",
    "    val_loss = val_loss_f + lambda_d * val_loss_d\n",
    "\n",
    "    # 還原參數的 requires_grad\n",
    "    for p in toggled:\n",
    "        p.requires_grad_(True)\n",
    "\n",
    "    train_losses.append(train_total)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # ===== Early stopping tracking =====\n",
    "    if val_loss < best_val - 1e-7:\n",
    "        best_val = val_loss\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        since_best = 0\n",
    "    else:\n",
    "        since_best += 1\n",
    "        if since_best >= patience:\n",
    "            # print(f\"Early stopped at epoch {ep}\")\n",
    "            break\n",
    "\n",
    "# ===== Restore best =====\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n",
    "# ===== Evaluate on test (function) =====\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(Xte).cpu().numpy()\n",
    "mse = float(np.mean((y_pred - y_test)**2))\n",
    "max_err = float(np.max(np.abs(y_pred - y_test)))\n",
    "print(f\"Test MSE      : {mse:.6e}\")\n",
    "print(f\"Test Max |err|: {max_err:.6e}\")\n",
    "\n",
    "# ===== (可選) Evaluate derivative on test =====\n",
    "# 如果你也要回報導數的 MSE：\n",
    "Xte_grad = Xte.detach().clone().requires_grad_(True)\n",
    "y_pred_te = model(Xte_grad)\n",
    "dte = torch.autograd.grad(y_pred_te, Xte_grad, torch.ones_like(y_pred_te))[0]\n",
    "mse_d = float(nn.MSELoss()(dte, fprime(Xte_grad)).item())\n",
    "max_err_d = float(torch.max(torch.abs(dte - fprime(Xte_grad))).item())\n",
    "print(f\"Test Derivative MSE      : {mse_d:.6e}\")\n",
    "print(f\"Test Derivative Max |err|: {max_err_d:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659662ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
