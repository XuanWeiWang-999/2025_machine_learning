{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e85096",
   "metadata": {},
   "source": [
    "# Problem 1. Explain the concept of score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67aa99c",
   "metadata": {},
   "source": [
    "## 1. Concept of score matching\n",
    "We usually hope to learn the **distribution model** of a data: \n",
    "$$p(x;\\theta)=\\frac{1}{Z(\\theta)}e^{-E(x;\\theta)},$$\n",
    "where $E(x;\\theta)$ is the energy function, and $Z(\\theta)=\\int e^{E(x;\\theta)}dx$ is a **patition function**.  \n",
    "\n",
    "In the sense of MLE, the goal is to maximum $$L(\\theta)=\\mathbb{E}_{q(x)}\\ln p(x;\\theta)$$  \n",
    "\n",
    "But usually in high-dimensional data set, $Z(\\theta)$ is almost not integrable.  \n",
    "\n",
    "Hence, the goal of ***Score Matching(Hyvärinen et.al, 2005)*** is **to estimate $E(x;\\theta)$ without knowing $Z(\\theta)$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2780a3d0",
   "metadata": {},
   "source": [
    "### Definition ---\n",
    "The **score function** of a probability distribution is defined as:\n",
    "\n",
    "$$\n",
    "\\psi(x; \\theta) = \\nabla_x \\log p(x; \\theta).\n",
    "$$\n",
    "\n",
    "It represents the direction in the data space along which the probability density\n",
    "increases most rapidly — in other words, it is a **gradient map of the data distribution**.\n",
    "\n",
    "The true data distribution  $q(x)$ also has its own score:\n",
    "\n",
    "$$\n",
    "\\nabla_x \\log q(x).\n",
    "$$\n",
    "\n",
    "The core idea of score matching is simple:\n",
    "\n",
    "> If the model’s score function equals the true data score function,  \n",
    "> then the two distributions share the same shape (up to a normalization constant).\n",
    "\n",
    "This means that by learning the gradient field $ \\psi(x; \\theta) $,  \n",
    "we can indirectly capture the geometry of the data density  \n",
    "without ever computing the intractable partition function $ Z(\\theta) $.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d083f2c",
   "metadata": {},
   "source": [
    "## 2. Denoising Score Matching (DSM)\n",
    "\n",
    "Vincent (2010) extended Hyvärinen’s score matching idea by introducing **denoising score matching (DSM)**.  \n",
    "The intuition is simple: if a data point is corrupted by Gaussian noise, a good model should learn a vector field\n",
    "that points from the noisy sample back toward the clean data.\n",
    "\n",
    "Assume a clean sample $x$ is perturbed by Gaussian noise:\n",
    "$$\n",
    "\\tilde{x} = x + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I).\n",
    "$$\n",
    "Then, the conditional distribution of the noisy sample is:\n",
    "$$\n",
    "q_\\sigma(\\tilde{x}\\,|\\,x)\n",
    "= \\frac{1}{(2\\pi\\sigma^2)^{d/2}}\n",
    "  \\exp\\!\\left(-\\frac{\\|\\tilde{x}-x\\|^2}{2\\sigma^2}\\right).\n",
    "$$\n",
    "The **denoising score matching objective** is:\n",
    "$$\n",
    "J_{\\text{DSM}}(\\theta)\n",
    "= \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\n",
    "  \\Bigg[\n",
    "  \\frac{1}{2}\n",
    "  \\Big\\|\n",
    "  \\psi(\\tilde{x};\\theta)\n",
    "  - \\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}\\,|\\,x)\n",
    "  \\Big\\|^2\n",
    "  \\Bigg].\n",
    "$$\n",
    "\n",
    "Since\n",
    "$$\n",
    "\\nabla_{\\tilde{x}} \\log q_\\sigma(\\tilde{x}\\,|\\,x)\n",
    "= \\frac{1}{\\sigma^2}(x - \\tilde{x}),\n",
    "$$\n",
    "the objective becomes:\n",
    "$$\n",
    "J_{\\text{DSM}}(\\theta)\n",
    "= \\mathbb{E}_{q_\\sigma(x, \\tilde{x})}\n",
    "  \\Bigg[\n",
    "  \\frac{1}{2}\n",
    "  \\Big\\|\n",
    "  \\psi(\\tilde{x};\\theta)\n",
    "  - \\frac{1}{\\sigma^2}(x - \\tilde{x})\n",
    "  \\Big\\|^2\n",
    "  \\Bigg].\n",
    "$$\n",
    "\n",
    "Thus, the network learns to predict the direction from a noisy point $\\tilde{x}$$\n",
    "back to the clean point $x$.\n",
    "Vincent showed that this objective is **mathematically equivalent**\n",
    "to training a **denoising autoencoder (DAE)** with squared reconstruction loss.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d9e77",
   "metadata": {},
   "source": [
    "## 3. Connection to Diffusion / Score-Based Generative Models\n",
    "\n",
    "Modern **score-based generative models** (Song & Ermon, 2019, 2020)\n",
    "extend DSM to multiple noise levels.\n",
    "Instead of adding noise once, data are progressively perturbed\n",
    "to create a sequence of noisy distributions $p_t(x)$, indexed by time $t$.\n",
    "\n",
    "A neural network $s_\\theta(x_t, t)$ is trained to approximate\n",
    "the score of each noisy distribution:\n",
    "$$\n",
    "s_\\theta(x_t, t) \\approx \\nabla_{x_t} \\log p_t(x_t).\n",
    "$$\n",
    "\n",
    "Training uses a weighted DSM loss over all noise levels:\n",
    "$$\n",
    "\\mathbb{E}_{t, x_0, \\epsilon}\n",
    "  \\Big[\n",
    "  \\lambda(t)\n",
    "  \\big\\|\n",
    "  s_\\theta(x_t, t)\n",
    "  - \\nabla_{x_t} \\log p_t(x_t|x_0)\n",
    "  \\big\\|^2\n",
    "  \\Big].\n",
    "$$\n",
    "\n",
    "Once trained, new samples are generated by **reverse diffusion**:\n",
    "starting from pure Gaussian noise and integrating\n",
    "a reverse-time stochastic differential equation (SDE)\n",
    "driven by the learned score function $s_\\theta(x_t, t)$.\n",
    "Hence, the model learns the gradient of log-density\n",
    "for all intermediate noise levels and can “walk back”\n",
    "from noise to data.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e6386b",
   "metadata": {},
   "source": [
    "## 4. Summary\n",
    "\n",
    "| Concept | Mathematical Formulation | Key Idea |\n",
    "|----------|--------------------------|-----------|\n",
    "| **Score Matching (SM)** | $ \\frac{1}{2}\\|\\psi - \\nabla \\log q\\|^2 $ | Learn the gradient (score) of the data density without computing normalization. |\n",
    "| **Implicit Score Matching (ISM)** | $ \\frac{1}{2}\\|\\psi\\|^2 + \\nabla_x \\!\\cdot\\! \\psi $ | Remove dependence on the true density $q$ via integration by parts. |\n",
    "| **Denoising Score Matching (DSM)** | $ \\frac{1}{2}\\|\\psi(\\tilde{x}) - (x-\\tilde{x})/\\sigma^2\\|^2 $ | Learn to move noisy samples back to clean data (equivalent to a DAE). |\n",
    "| **Diffusion Models** | $ s_\\theta(x_t,t) \\approx \\nabla_{x_t}\\log p_t(x_t) $ | Extend DSM to a continuous noise schedule; reverse diffusion generates samples. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f5557f",
   "metadata": {},
   "source": [
    "> *Reference: Vincent, P. (2010). “A Connection Between Score Matching and Denoising Autoencoders.”*\n",
    "> *Technical Report 1358, Université de Montréal.*  \n",
    "> **Remark:** The original English phrasing and order were revised with assistance from ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7669115c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
