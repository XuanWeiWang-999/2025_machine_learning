{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315654f0",
   "metadata": {},
   "source": [
    "# Unanswered Question - Week 1-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6b239",
   "metadata": {},
   "source": [
    "## Week 1  \n",
    "### Questions\n",
    "Noticed that the update formula for **gradient descent** sometimes includes different coefficients, such as:\n",
    "\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n - \\alpha \\nabla L(\\theta)\n",
    "$$\n",
    "vs.\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n + \\frac{2\\alpha}{m} \\sum (y^{(i)} - h(x^{(i)})) \\nabla_\\theta h\n",
    "$$\n",
    "\n",
    "這邊的問題是：\n",
    "* 為甚麼公式看起來有些不一樣? 是因為只差在$\\frac{1}{2}$的factor而已嗎?\n",
    "* 如果一些常數可以被$\\alpha$自然吸收掉，那是不是就意味著那些常數本來就不重要?反正只要選一個\"好的\"$\\alpha$就好?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21965e",
   "metadata": {},
   "source": [
    "### Answer\n",
    "### 1. 為什麼兩個公式看起來不一樣？\n",
    "因為損失函數的定義不同，特別是 **有沒有放 1/2**。  \n",
    "若沒有放 1/2，梯度會多一個 2；若放了 1/2，這個 2 會被抵消，所以更新式看起來不同。\n",
    "\n",
    "\n",
    "### 2. 這些常數重要嗎？\n",
    "確實在實作當中不重要，因為它們都可以被 **學習率 α 吸收**。  \n",
    "真正影響收斂的是 α 的選擇，而不是前面的 2 或 1/2。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c92ca",
   "metadata": {},
   "source": [
    "### References\n",
    "* Ian Goodfellow, Yoshua Bengio,  Aaron Courville, *Deep Learning*,chapter 4.3.  \n",
    "* beihangzxm123, *Machine Learning - Andrew Ng on Coursera (Week 2)*, 2016.03.28, [網路資料](https://blog.csdn.net/qq_26898461/article/details/50995810)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633dddfd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2\n",
    "### Question\n",
    "MLE maximizes training log-likelihood. With high-capacity models or small/noisy data, it fits idiosyncrasies → poor generalization and over-confident probabilities.  \n",
    "Is there any good method to solve this kind of overfitting problem?\n",
    "\n",
    "「MLE 會最大化訓練資料的對數似然。在模型容量很大、或資料量太小／噪音太多時，它會去擬合資料中的偶然特性（idiosyncrasies），導致泛化能力變差、預測機率過度自信。\n",
    "那有沒有什麼好方法可以解決這類過擬合的問題呢？」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7615c2",
   "metadata": {},
   "source": [
    "#### 如何解決 MLE 過擬合（overfitting）問題？\n",
    "\n",
    "1. **Regularization（正則化）**  \n",
    "   在 log-likelihood 上加入 penalty，例如 L2（weight decay）、L1。  \n",
    "   → 控制模型複雜度，阻止參數無限放大。\n",
    "\n",
    "2. **Bayesian Methods（貝葉斯方法）**  \n",
    "   MLE 換成 MAP，加 prior；或使用 variational inference。  \n",
    "   → 讓模型不會出現過度自信的 posterior。\n",
    "\n",
    "3. **Early Stopping（提前停止）**  \n",
    "   觀察 validation loss，一旦開始上升就停止訓練。\n",
    "\n",
    "4. **Dropout / Noise Injection（隱藏單元丟棄、加入噪音）**  \n",
    "   常用於 deep models，可視為 implicit regularization。\n",
    "\n",
    "5. **Data Augmentation（資料增強）**  \n",
    "   對影像、語音或 NLP 特別有效，提高泛化。\n",
    "\n",
    "6. **Ensemble（集成方法）**  \n",
    "   Bagging、Random Forest、Boosting、Deep Ensembles。  \n",
    "   → 能有效降低過度自信的預測。\n",
    "\n",
    "7. **Label Smoothing（標籤平滑）**  \n",
    "   特別在分類模型能降低過度自信的 softmax 輸出。\n",
    "\n",
    "8. **Temperature Scaling / Calibration（機率校準）**  \n",
    "   減少 MLE 產生的 over-confident probabilities。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766b56d",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Christopher Bishop, *Pattern Recognition and Machine Learning*, Chapter 1.5 & 3.  \n",
    "   — 討論 MLE、MAP 與 overfitting 的關係。\n",
    "\n",
    "2. Ian Goodfellow, Yoshua Bengio, Aaron Courville,  \n",
    "   *Deep Learning*, Chapter 5 & 7.  \n",
    "   — 詳述 regularization 與各種防止過擬合的方法。\n",
    "\n",
    "3. Andrew Ng, CS229 Lecture Notes (Supervised Learning).  \n",
    "   — 清楚說明為何 MLE 會過擬合以及 regularization 的效果。\n",
    "\n",
    "4. \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\",  \n",
    "   Srivastava et al., JMLR 2014.\n",
    "\n",
    "5. \"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\",  \n",
    "   Lakshminarayanan et al., NIPS 2017.  \n",
    "   — 深度模型 over-confidence 的經典解法。\n",
    "\n",
    "6. \"On Calibration of Modern Neural Networks\",  \n",
    "   Guo et al., ICML 2017.  \n",
    "   — 提出 temperature scaling。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4cd86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186d067",
   "metadata": {},
   "source": [
    "## Week 3\n",
    "### Question\n",
    "In the reference paper, in both Lemma 3.1 and Lemma 3.2, the construction shows that shallow tanh networks can approximate monomials with arbitrary accuracy. However, the required weights often grow rapidly as the error tolerance $\\varepsilon \\to 0$ or as the polynomial degree $s$ increases.\n",
    "\n",
    "What are the implications of such large weights in practice? For instance, could excessively large weights lead to numerical instability, gradient explosion or vanishing during training, or poor generalization?\n",
    "\n",
    "How might one balance theoretical approximation guarantees with practical considerations in optimization?\n",
    "\n",
    "在參考的論文中，Lemma 3.1 與 Lemma 3.2 的構造顯示：淺層的 tanh 網路可以以任意精度逼近單項式。然而，當誤差容許度 \n",
    "$ \\varepsilon \\to 0 $或多項式階數 $s$ 增加時，所需的權重會快速變大。\n",
    "\n",
    "這樣巨大的權重在實務上會造成什麼影響？例如，過大的權重是否可能導致數值不穩定、gradient爆炸或不見、或generalization的能力下降？\n",
    "\n",
    "又該如何在「理論上的函數逼近保證」與「實際可行的最佳化與訓練」之間取得平衡？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485d4b0",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "#### 1. 巨大權重的影響\n",
    "##### 1.1 數值不穩定（Numerical Instability）\n",
    "- 權重越大，網路輸出越容易超出硬體float的範圍 ，造成 overflow。\n",
    "- 在 tanh 或 sigmoid 中，大input會使 activations 飽和，導致梯度極小。\n",
    "\n",
    "##### 1.2 梯度爆炸與梯度消失（Gradient Explosion / Vanishing）\n",
    "- 大權重會放大誤差與gradient，造成訓練不穩定。  \n",
    "- 飽和值區會讓梯度消失，導致網路無法更新。\n",
    "\n",
    "##### 1.3 泛化能力變差（Poor Generalization）\n",
    "- 巨大權重常意味網路對數據分布太敏感，容易 overfitting。  \n",
    "- 實務上，平滑、較小的權重通常帶來更好的泛化表現。\n",
    "\n",
    "##### 1.4 最佳化困難\n",
    "- Loss landscape 會變得陡峭或不平滑，讓 SGD / Adam 更難找到良好收斂點。\n",
    "\n",
    "#### 2. 如何在理論與實務之間取得平衡?\n",
    "##### 2.1 加入 Regularization\n",
    "- **L2 regularization（weight decay）** 可直接限制權重大小。  \n",
    "- **L1** 也能強制 sparsity 並間接減少極端權重。\n",
    "\n",
    "公式示意：  \n",
    "\\[\n",
    "\\min_\\theta L(\\theta) + \\lambda \\|\\theta\\|_2^2\n",
    "\\]\n",
    "\n",
    "##### 2.2 使用更適合逼近的 Activation（如 ReLU, GELU）\n",
    "某些 activation（如 tanh）在輸入大時容易飽和，  \n",
    "若改用 ReLU 類激活，可降低對巨大權重的需求。\n",
    "\n",
    "文獻中也指出 ReLU 的逼近效率通常比 tanh 更有利。\n",
    "\n",
    "##### 2.3 使用更深的模型（Depth > Width）\n",
    "深度網路的理論（如 Telgarsky、Eldan-Shamir）指出：  \n",
    "**深度能以較少權重、更平緩的參數達到同等逼近能力。**\n",
    "\n",
    "※ 深度減少巨大權重的必要性  \n",
    "※ 深度增加 representation efficiency\n",
    "\n",
    "##### 2.4 正則化初始化與 Normalization\n",
    "- Xavier / Kaiming initialization 限制初始權重範圍  \n",
    "- BatchNorm / LayerNorm 防止訊號在 forward/backward 擴散失控\n",
    "\n",
    "##### 2.5 控制逼近目標範圍（Rescaling）\n",
    "許多理論需要巨大權重，是因為要在整個 domain 上逼近非常大的函數。  \n",
    "實務上可：\n",
    "\n",
    "- 對目標函數 normalize  \n",
    "- 對輸入資料 rescale  \n",
    "\n",
    "降低需要的權重大小。\n",
    "\n",
    "##### 2.6 使用 Bayesian 方法\n",
    "以 prior 限制權重，使模型不會出現無限制的大參數。\n",
    "\n",
    "\\[\n",
    "p(\\theta\\mid \\text{data}) \\propto p(\\text{data}\\mid\\theta) p(\\theta)\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3863c",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "#### (A) Function Approximation / Theory\n",
    "1. Mhaskar, H. N., & Poggio, T. (2016). *Deep vs. shallow networks*.  \n",
    "   — 說明深度網路可用更少的參數逼近複雜函數。\n",
    "\n",
    "2. Barron, A. (1993). *Universal Approximation Bounds for Superpositions of a Sigmoidal Function*.  \n",
    "   — 探討權重大小與逼近誤差的關係。\n",
    "\n",
    "3. Yarotsky, D. (2017). *Error bounds for approximations with ReLU networks*.  \n",
    "   — ReLU 神經網路的逼近效率及權重界限。\n",
    "\n",
    "4. Pinkus, A. (1999). *Approximation theory of the MLP model*.  \n",
    "   — 經典：討論多層感知機逼近的權重、深度與誤差。\n",
    "\n",
    "#### (B) Optimization & Generalization\n",
    "5. Goodfellow, Bengio, Courville — *Deep Learning* (Chapter 7, 8)  \n",
    "   — 討論大權重對訓練與泛化的影響。\n",
    "\n",
    "6. Neyshabur et al. (2015). *Norm-based capacity control*.  \n",
    "   — 權重的大小對 generalization 有直接關聯。\n",
    "\n",
    "7. Zhang et al. (2017). *Understanding deep learning requires rethinking generalization*.  \n",
    "   — 指出大權重模型可能記住雜訊並導致過擬合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91211cb4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b836a",
   "metadata": {},
   "source": [
    "## Week 4\n",
    "### Question\n",
    "When the number of categories is very large (e.g., tens of thousands of words in NLP), one-hot encoding produces extremely high-dimensional sparse vectors. This leads to inefficient memory usage and difficulties in capturing meaningful relationships between categories.  \n",
    "\n",
    "Or maybe in implementation, we usually don't use one-hot encoding?  \n",
    "\n",
    "當類別數量非常大時（例如 NLP 裡有成千上萬個單字），one-hot 編碼會產生極高維、且非常稀疏的向量。這會造成記憶體使用效率低落，也不容易捕捉類別之間有意義的關係。  \n",
    "\n",
    "或者，其實在實作中我們通常並不會真的用 one-hot 編碼？  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59b25f",
   "metadata": {},
   "source": [
    "### Answer：實務上真的會用 one-hot 嗎？\n",
    "\n",
    "當類別數非常大（例如 NLP 裡的字詞字典有幾萬、幾十萬個詞）時，**理論上會用 one-hot 向量來表示類別，但實作中通常不會真的建立這個超高維稀疏向量**，原因包括：\n",
    "\n",
    "1. **記憶體與計算效率很差**  \n",
    "   - 真正的 one-hot 是長度為 `V`（字典大小）的向量，大部分都是 0，只有一個位置是 1。  \n",
    "   - 若 `V = 50,000` 或 `100,000`，每個樣本都存這麼長的向量非常浪費記憶體與計算量。\n",
    "\n",
    "2. **無法表示類別之間的語意關係**  \n",
    "   - 在 one-hot 表示中，每個類別之間的距離都一樣（例如 cos 距離、L2 距離），無法表達「dog 比 car 更接近 cat」這種結構。  \n",
    "   - 也就是說，**one-hot 只表示「是不是同一類」而不是「多像」或「語意多接近」。**\n",
    "\n",
    "3. **實務上常用的是「embedding lookup」，而不是explicit one-hot**  \n",
    "   - 在程式裡，我們通常把每個類別（例如單字）表示成一個整數 id，然後透過 **embedding matrix** 去查表拿到一個低維稠密向量。  \n",
    "   - 這個步驟在數學上等價於：  \n",
    "     先做 one-hot，再乘上 embedding matrix  \n",
    "     但實作時**不會真的把 one-hot 生出來**，而是直接用 index 查表，節省很多計算。\n",
    "   - PyTorch 的 `nn.Embedding`、TensorFlow/Keras 的 `Embedding` layer 就是這種典型做法。\n",
    "\n",
    "4. **embedding 還可以透過訓練學到語意結構**  \n",
    "   - 如 word2vec、GloVe、fastText 等方法，會讓「語意接近的詞」在 embedding 空間中距離也比較近，這是 one-hot 完全做不到的。\n",
    "\n",
    "**所以總結：**\n",
    "\n",
    "- 理論上可以把類別想成 one-hot；  \n",
    "- 但**實務上幾乎不會顯式建立 one-hot 向量**，而是：  \n",
    "  - 用「整數 id」＋「embedding lookup」來實現，  \n",
    "  - 得到維度較低、可訓練、能表示語意關係的向量。\n",
    "  \n",
    "\n",
    "### References\n",
    "\n",
    "1. Ian Goodfellow, Yoshua Bengio, Aaron Courville,  \n",
    "   *Deep Learning*, MIT Press, Chapter 12 (Distributed Representations and Language Models).  \n",
    "   - 討論為何要用分佈式表示（embeddings）而不是 one-hot，特別是在語言模型中的動機。\n",
    "\n",
    "2. Tianqi Chen and Mu Li,  \n",
    "   *Introduction to Word Embedding*, in *Dive into Deep Learning* (D2L), NLP chapters.  \n",
    "   - 說明從 one-hot 到 embedding 的轉換，並展示實作上如何用 embedding 矩陣代替 one-hot。\n",
    "\n",
    "3. Tomas Mikolov et al.,  \n",
    "   *Efficient Estimation of Word Representations in Vector Space* (word2vec).  \n",
    "   - 經典論文，展示如何學習 word embeddings，動機之一就是避免高維 one-hot 表示的缺點。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15d2c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566651ea",
   "metadata": {},
   "source": [
    "## Week 5\n",
    "### Question\n",
    "We observe i.i.d. samples $(x_i, \\tilde y_i)$, where the true label  \n",
    "$y \\in \\{0,1\\}$ is flipped by an **unknown** noise/confusion matrix $T$.  \n",
    "The generative model is:\n",
    "\n",
    "$$\n",
    "y \\sim \\text{Bern}(\\pi), \\qquad x \\mid y=k \\sim \\mathcal N(\\mu_k, \\Sigma_k),\n",
    "$$\n",
    "$$\n",
    "\\Pr(\\tilde y=j \\mid y=k) = T_{jk}.\n",
    "$$\n",
    "\n",
    "Thus the observed likelihood is:\n",
    "\n",
    "$$\n",
    "p(x, \\tilde y = j)\n",
    "  = \\pi_0 T_{j0}\\, \\mathcal N_0(x)\n",
    "  + \\pi_1 T_{j1}\\, \\mathcal N_1(x).\n",
    "$$\n",
    "\n",
    "**Question:**  \n",
    "From only $p(x, \\tilde y)$, are the parameters  \n",
    "$\\{\\pi, \\mu_0, \\mu_1, \\Sigma_0, \\Sigma_1\\}$ and the unknown $T$  \n",
    "**identifiable up to label swapping**?  \n",
    "If not, what are the *minimal assumptions* needed for identifiability  \n",
    "(e.g., invertible $T$, distinct Gaussian components, or small side information)?\n",
    "\n",
    "\n",
    "我們觀察到 i.i.d. 的資料 $(x_i, \\tilde y_i)$，其中真實標籤  \n",
    "$y \\in \\{0,1\\}$ 會被未知的雜訊矩陣 $T$ 翻轉。模型為：\n",
    "\n",
    "$$\n",
    "y \\sim \\text{Bern}(\\pi),\\qquad x \\mid y=k \\sim \\mathcal N(\\mu_k, \\Sigma_k),\n",
    "$$\n",
    "$$\n",
    "\\Pr(\\tilde y=j \\mid y=k) = T_{jk}.\n",
    "$$\n",
    "\n",
    "因此我們看到的分佈是：兩個 Gaussian component 相同、但混合權重未知的 mixture。\n",
    "\n",
    "**問題：**  \n",
    "僅從 $p(x, \\tilde y)$ 能不能唯一決定  \n",
    "$\\{\\pi, \\mu_0, \\mu_1, \\Sigma_0, \\Sigma_1\\}$ 和未知的 $T$？  \n",
    "如果不能，則能做到需要的最弱的條件是甚麼？例如：\n",
    "\n",
    "- 雜訊矩陣 $T$ 可逆、對角優勢  \n",
    "- 兩個 Gaussian component 不相同  \n",
    "- 或需要少量乾淨資料、已知先驗、或雜訊率界限等額外資訊。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac5202",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "要讓 $\\{ \\pi,\\mu_0,\\mu_1,\\Sigma_0,\\Sigma_1 \\}$ 與未知的雜訊矩陣 $T$  \n",
    "能從 $p(x,\\tilde y)$ 中被唯一（最多差 label swap）識別，需要以下最小條件：\n",
    "\n",
    "\n",
    "##### **1. Gaussian 組件必須不同（Distinct Components）**\n",
    "若兩個 component 完全相同：\n",
    "$$\n",
    "(\\mu_0,\\Sigma_0) = (\\mu_1,\\Sigma_1),\n",
    "$$\n",
    "那無論 $T$ 是什麼，觀察到的分佈都會一樣，完全無法分辨類別。\n",
    "\n",
    "**最小條件：**\n",
    "$$\n",
    "\\mathcal N(\\mu_0,\\Sigma_0) \\neq \\mathcal N(\\mu_1,\\Sigma_1).\n",
    "$$\n",
    "\n",
    "##### **2. 雜訊矩陣 $T$ 必須可逆（Invertible）**\n",
    "若 $T$ 不可逆（例如兩行相同），則不同的 $(\\pi,\\mu_k,\\Sigma_k)$  \n",
    "可以透過不同的 $T$ 得到同樣的 $p(x,\\tilde y)$，無法唯一辨識。\n",
    "\n",
    "**最小條件：**\n",
    "$$\n",
    "\\det(T) \\neq 0.\n",
    "$$\n",
    "\n",
    "##### **3. 雜訊不能太大（Diagonal Dominance）**\n",
    "若 $T$ 幾乎完全把 0 變 1、1 變 0（或接近全亂數），  \n",
    "那 $(\\pi, T)$ 的組合會高度不識別。\n",
    "\n",
    "通常需要：\n",
    "$$\n",
    "T_{00}, T_{11} > \\tfrac{1}{2}.\n",
    "$$\n",
    "\n",
    "這確保「看到的標籤」還保留部分真實訊息。\n",
    "\n",
    "##### **4. 兩個 mixture（對不同 $\\tilde y$）的 mixing weight 足夠不同**\n",
    "對每個 observed label $j$：\n",
    "$$\n",
    "p(x \\mid \\tilde y=j) = c_{j0} \\mathcal N_0(x) + c_{j1} \\mathcal N_1(x)\n",
    "$$\n",
    "\n",
    "兩個 mixture 必須提供足夠的線性獨立資訊來解回：\n",
    "- component 分布 (Gaussian 參數)\n",
    "- mixing weight（含 $\\pi$ 與 $T$）\n",
    "\n",
    "這在 $T$ 可逆時成立。\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "#### **(A) Mixture Model Identifiability**\n",
    "1. Teicher, H. (1963). *Identifiability of finite mixtures.*  \n",
    "   — 經典定理：Gaussian mixtures 在組件不同時是可識別的。\n",
    "\n",
    "2. Yakowitz & Spragins (1968). *On the identifiability of mixtures.*  \n",
    "   — 多種 mixture 模型的可識別性條件。\n",
    "\n",
    "3. Allman, Matias, Rhodes (2009). *Identifiability of latent structure models with many observed variables.*  \n",
    "   — 混合模型 + 隱變數的可識別性，技術上最強。\n",
    "\n",
    "\n",
    "#### **(B) Label Noise / Confusion Matrix Identifiability**\n",
    "4. Scott, C. (2015). *A Rate of Convergence for Mixture Proportion Estimation.*  \n",
    "   — 介紹類別雜訊矩陣的可識別條件。\n",
    "\n",
    "5. Natarajan et al. (2013). *Learning with Noisy Labels.*  \n",
    "   — 使用可逆的 noise matrix 來校正 loss 的基礎模型。\n",
    "\n",
    "6. Patrini et al. (2017). *Making Deep Neural Networks Robust to Label Noise.*  \n",
    "   — 討論 confusion matrix 可識別性與 invertibility 的必要性。\n",
    "\n",
    "#### **(C) Gaussian Discriminant Analysis with Label Noise**\n",
    "7. Frénay & Verleysen (2014). *Classification in the presence of label noise.*  \n",
    "   — 對 binary noise model 的 identifiability 有完整整理。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a9e5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb80243",
   "metadata": {},
   "source": [
    "## Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca0c8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8d3247c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af8b083",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c005c2e",
   "metadata": {},
   "source": [
    "## Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8989a23",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
