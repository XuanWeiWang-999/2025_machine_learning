{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315654f0",
   "metadata": {},
   "source": [
    "# Unanswered Question - Week 1-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6b239",
   "metadata": {},
   "source": [
    "## Week 1  \n",
    "### Questions\n",
    "Noticed that the update formula for **gradient descent** sometimes includes different coefficients, such as:\n",
    "\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n - \\alpha \\nabla L(\\theta)\n",
    "$$\n",
    "vs.\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n + \\frac{2\\alpha}{m} \\sum (y^{(i)} - h(x^{(i)})) \\nabla_\\theta h\n",
    "$$\n",
    "\n",
    "這邊的問題是：\n",
    "* 為甚麼公式看起來有些不一樣? 是因為只差在$\\frac{1}{2}$的factor而已嗎?\n",
    "* 如果一些常數可以被$\\alpha$自然吸收掉，那是不是就意味著那些常數本來就不重要?反正只要選一個\"好的\"$\\alpha$就好?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21965e",
   "metadata": {},
   "source": [
    "### Answer\n",
    "### 1. 為什麼兩個公式看起來不一樣？\n",
    "因為損失函數的定義不同，特別是 **有沒有放 1/2**。  \n",
    "若沒有放 1/2，梯度會多一個 2；若放了 1/2，這個 2 會被抵消，所以更新式看起來不同。\n",
    "\n",
    "\n",
    "### 2. 這些常數重要嗎？\n",
    "確實在實作當中不重要，因為它們都可以被 **學習率 α 吸收**。  \n",
    "真正影響收斂的是 α 的選擇，而不是前面的 2 或 1/2。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c92ca",
   "metadata": {},
   "source": [
    "### References\n",
    "* Ian Goodfellow, Yoshua Bengio,  Aaron Courville, *Deep Learning*,chapter 4.3.  \n",
    "* beihangzxm123, *Machine Learning - Andrew Ng on Coursera (Week 2)*, 2016.03.28, [網路資料](https://blog.csdn.net/qq_26898461/article/details/50995810)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633dddfd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7615c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
