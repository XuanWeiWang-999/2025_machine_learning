{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315654f0",
   "metadata": {},
   "source": [
    "# Unanswered Question - Week 1-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd6b239",
   "metadata": {},
   "source": [
    "## Week 1  \n",
    "### Questions\n",
    "Noticed that the update formula for **gradient descent** sometimes includes different coefficients, such as:\n",
    "\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n - \\alpha \\nabla L(\\theta)\n",
    "$$\n",
    "vs.\n",
    "$$\n",
    "\\theta^{n+1} = \\theta^n + \\frac{2\\alpha}{m} \\sum (y^{(i)} - h(x^{(i)})) \\nabla_\\theta h\n",
    "$$\n",
    "\n",
    "這邊的問題是：\n",
    "* 為甚麼公式看起來有些不一樣? 是因為只差在$\\frac{1}{2}$的factor而已嗎?\n",
    "* 如果一些常數可以被$\\alpha$自然吸收掉，那是不是就意味著那些常數本來就不重要?反正只要選一個\"好的\"$\\alpha$就好?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae21965e",
   "metadata": {},
   "source": [
    "### Answer\n",
    "### 1. 為什麼兩個公式看起來不一樣？\n",
    "因為損失函數的定義不同，特別是 **有沒有放 1/2**。  \n",
    "若沒有放 1/2，梯度會多一個 2；若放了 1/2，這個 2 會被抵消，所以更新式看起來不同。\n",
    "\n",
    "\n",
    "### 2. 這些常數重要嗎？\n",
    "確實在實作當中不重要，因為它們都可以被 **學習率 α 吸收**。  \n",
    "真正影響收斂的是 α 的選擇，而不是前面的 2 或 1/2。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3c92ca",
   "metadata": {},
   "source": [
    "### References\n",
    "* Ian Goodfellow, Yoshua Bengio,  Aaron Courville, *Deep Learning*,chapter 4.3.  \n",
    "* beihangzxm123, *Machine Learning - Andrew Ng on Coursera (Week 2)*, 2016.03.28, [網路資料](https://blog.csdn.net/qq_26898461/article/details/50995810)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633dddfd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2\n",
    "### Question\n",
    "MLE maximizes training log-likelihood. With high-capacity models or small/noisy data, it fits idiosyncrasies → poor generalization and over-confident probabilities.  \n",
    "Is there any good method to solve this kind of overfitting problem?\n",
    "\n",
    "「MLE 會最大化訓練資料的對數似然。在模型容量很大、或資料量太小／噪音太多時，它會去擬合資料中的偶然特性（idiosyncrasies），導致泛化能力變差、預測機率過度自信。\n",
    "那有沒有什麼好方法可以解決這類過擬合的問題呢？」"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7615c2",
   "metadata": {},
   "source": [
    "#### 如何解決 MLE 過擬合（overfitting）問題？\n",
    "\n",
    "1. **Regularization（正則化）**  \n",
    "   在 log-likelihood 上加入 penalty，例如 L2（weight decay）、L1。  \n",
    "   → 控制模型複雜度，阻止參數無限放大。\n",
    "\n",
    "2. **Bayesian Methods（貝葉斯方法）**  \n",
    "   MLE 換成 MAP，加 prior；或使用 variational inference。  \n",
    "   → 讓模型不會出現過度自信的 posterior。\n",
    "\n",
    "3. **Early Stopping（提前停止）**  \n",
    "   觀察 validation loss，一旦開始上升就停止訓練。\n",
    "\n",
    "4. **Dropout / Noise Injection（隱藏單元丟棄、加入噪音）**  \n",
    "   常用於 deep models，可視為 implicit regularization。\n",
    "\n",
    "5. **Data Augmentation（資料增強）**  \n",
    "   對影像、語音或 NLP 特別有效，提高泛化。\n",
    "\n",
    "6. **Ensemble（集成方法）**  \n",
    "   Bagging、Random Forest、Boosting、Deep Ensembles。  \n",
    "   → 能有效降低過度自信的預測。\n",
    "\n",
    "7. **Label Smoothing（標籤平滑）**  \n",
    "   特別在分類模型能降低過度自信的 softmax 輸出。\n",
    "\n",
    "8. **Temperature Scaling / Calibration（機率校準）**  \n",
    "   減少 MLE 產生的 over-confident probabilities。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2766b56d",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. Christopher Bishop, *Pattern Recognition and Machine Learning*, Chapter 1.5 & 3.  \n",
    "   — 討論 MLE、MAP 與 overfitting 的關係。\n",
    "\n",
    "2. Ian Goodfellow, Yoshua Bengio, Aaron Courville,  \n",
    "   *Deep Learning*, Chapter 5 & 7.  \n",
    "   — 詳述 regularization 與各種防止過擬合的方法。\n",
    "\n",
    "3. Andrew Ng, CS229 Lecture Notes (Supervised Learning).  \n",
    "   — 清楚說明為何 MLE 會過擬合以及 regularization 的效果。\n",
    "\n",
    "4. \"Dropout: A Simple Way to Prevent Neural Networks from Overfitting\",  \n",
    "   Srivastava et al., JMLR 2014.\n",
    "\n",
    "5. \"Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles\",  \n",
    "   Lakshminarayanan et al., NIPS 2017.  \n",
    "   — 深度模型 over-confidence 的經典解法。\n",
    "\n",
    "6. \"On Calibration of Modern Neural Networks\",  \n",
    "   Guo et al., ICML 2017.  \n",
    "   — 提出 temperature scaling。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b4cd86",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186d067",
   "metadata": {},
   "source": [
    "## Week 3\n",
    "### Question\n",
    "In the reference paper, in both Lemma 3.1 and Lemma 3.2, the construction shows that shallow tanh networks can approximate monomials with arbitrary accuracy. However, the required weights often grow rapidly as the error tolerance $\\varepsilon \\to 0$ or as the polynomial degree $s$ increases.\n",
    "\n",
    "What are the implications of such large weights in practice? For instance, could excessively large weights lead to numerical instability, gradient explosion or vanishing during training, or poor generalization?\n",
    "\n",
    "How might one balance theoretical approximation guarantees with practical considerations in optimization?\n",
    "\n",
    "在參考的論文中，Lemma 3.1 與 Lemma 3.2 的構造顯示：淺層的 tanh 網路可以以任意精度逼近單項式。然而，當誤差容許度 \n",
    "$ \\varepsilon \\to 0 $或多項式階數 $s$ 增加時，所需的權重會快速變大。\n",
    "\n",
    "這樣巨大的權重在實務上會造成什麼影響？例如，過大的權重是否可能導致數值不穩定、gradient爆炸或不見、或generalization的能力下降？\n",
    "\n",
    "又該如何在「理論上的函數逼近保證」與「實際可行的最佳化與訓練」之間取得平衡？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3485d4b0",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "#### 1. 巨大權重的影響\n",
    "##### 1.1 數值不穩定（Numerical Instability）\n",
    "- 權重越大，網路輸出越容易超出硬體float的範圍 ，造成 overflow。\n",
    "- 在 tanh 或 sigmoid 中，大input會使 activations 飽和，導致梯度極小。\n",
    "\n",
    "##### 1.2 梯度爆炸與梯度消失（Gradient Explosion / Vanishing）\n",
    "- 大權重會放大誤差與gradient，造成訓練不穩定。  \n",
    "- 飽和值區會讓梯度消失，導致網路無法更新。\n",
    "\n",
    "##### 1.3 泛化能力變差（Poor Generalization）\n",
    "- 巨大權重常意味網路對數據分布太敏感，容易 overfitting。  \n",
    "- 實務上，平滑、較小的權重通常帶來更好的泛化表現。\n",
    "\n",
    "##### 1.4 最佳化困難\n",
    "- Loss landscape 會變得陡峭或不平滑，讓 SGD / Adam 更難找到良好收斂點。\n",
    "\n",
    "#### 2. 如何在理論與實務之間取得平衡?\n",
    "##### 2.1 加入 Regularization\n",
    "- **L2 regularization（weight decay）** 可直接限制權重大小。  \n",
    "- **L1** 也能強制 sparsity 並間接減少極端權重。\n",
    "\n",
    "公式示意：  \n",
    "\\[\n",
    "\\min_\\theta L(\\theta) + \\lambda \\|\\theta\\|_2^2\n",
    "\\]\n",
    "\n",
    "##### 2.2 使用更適合逼近的 Activation（如 ReLU, GELU）\n",
    "某些 activation（如 tanh）在輸入大時容易飽和，  \n",
    "若改用 ReLU 類激活，可降低對巨大權重的需求。\n",
    "\n",
    "文獻中也指出 ReLU 的逼近效率通常比 tanh 更有利。\n",
    "\n",
    "##### 2.3 使用更深的模型（Depth > Width）\n",
    "深度網路的理論（如 Telgarsky、Eldan-Shamir）指出：  \n",
    "**深度能以較少權重、更平緩的參數達到同等逼近能力。**\n",
    "\n",
    "※ 深度減少巨大權重的必要性  \n",
    "※ 深度增加 representation efficiency\n",
    "\n",
    "##### 2.4 正則化初始化與 Normalization\n",
    "- Xavier / Kaiming initialization 限制初始權重範圍  \n",
    "- BatchNorm / LayerNorm 防止訊號在 forward/backward 擴散失控\n",
    "\n",
    "##### 2.5 控制逼近目標範圍（Rescaling）\n",
    "許多理論需要巨大權重，是因為要在整個 domain 上逼近非常大的函數。  \n",
    "實務上可：\n",
    "\n",
    "- 對目標函數 normalize  \n",
    "- 對輸入資料 rescale  \n",
    "\n",
    "降低需要的權重大小。\n",
    "\n",
    "##### 2.6 使用 Bayesian 方法\n",
    "以 prior 限制權重，使模型不會出現無限制的大參數。\n",
    "\n",
    "\\[\n",
    "p(\\theta\\mid \\text{data}) \\propto p(\\text{data}\\mid\\theta) p(\\theta)\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3863c",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "#### (A) Function Approximation / Theory\n",
    "1. Mhaskar, H. N., & Poggio, T. (2016). *Deep vs. shallow networks*.  \n",
    "   — 說明深度網路可用更少的參數逼近複雜函數。\n",
    "\n",
    "2. Barron, A. (1993). *Universal Approximation Bounds for Superpositions of a Sigmoidal Function*.  \n",
    "   — 探討權重大小與逼近誤差的關係。\n",
    "\n",
    "3. Yarotsky, D. (2017). *Error bounds for approximations with ReLU networks*.  \n",
    "   — ReLU 神經網路的逼近效率及權重界限。\n",
    "\n",
    "4. Pinkus, A. (1999). *Approximation theory of the MLP model*.  \n",
    "   — 經典：討論多層感知機逼近的權重、深度與誤差。\n",
    "\n",
    "#### (B) Optimization & Generalization\n",
    "5. Goodfellow, Bengio, Courville — *Deep Learning* (Chapter 7, 8)  \n",
    "   — 討論大權重對訓練與泛化的影響。\n",
    "\n",
    "6. Neyshabur et al. (2015). *Norm-based capacity control*.  \n",
    "   — 權重的大小對 generalization 有直接關聯。\n",
    "\n",
    "7. Zhang et al. (2017). *Understanding deep learning requires rethinking generalization*.  \n",
    "   — 指出大權重模型可能記住雜訊並導致過擬合。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91211cb4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b836a",
   "metadata": {},
   "source": [
    "## Week 4\n",
    "### Question\n",
    "When the number of categories is very large (e.g., tens of thousands of words in NLP), one-hot encoding produces extremely high-dimensional sparse vectors. This leads to inefficient memory usage and difficulties in capturing meaningful relationships between categories.  \n",
    "\n",
    "Or maybe in implementation, we usually don't use one-hot encoding?  \n",
    "\n",
    "當類別數量非常大時（例如 NLP 裡有成千上萬個單字），one-hot 編碼會產生極高維、且非常稀疏的向量。這會造成記憶體使用效率低落，也不容易捕捉類別之間有意義的關係。  \n",
    "\n",
    "或者，其實在實作中我們通常並不會真的用 one-hot 編碼？  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59b25f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90bb73ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40adb726",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d15d2c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566651ea",
   "metadata": {},
   "source": [
    "## Week 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac5202",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3b2f4fd6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2da28139",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c4a9e5d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb80243",
   "metadata": {},
   "source": [
    "## Week 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca0c8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8d3247c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af8b083",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c005c2e",
   "metadata": {},
   "source": [
    "## Week 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8989a23",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
